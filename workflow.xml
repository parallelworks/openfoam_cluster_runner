<tool id='User.Demo_say_hello' name='User.Demo_say_hello'>
  <command interpreter='bash'>main.sh</command>
  <inputs>
   <section name='openfoam' type='section' title='OpenFOAM' expanded='true'>
        <param name='_pw_cases_json_file' label='JSON ases definition in remote host' type='text' value='__poolworkdir__/pw/openfoam/cyclone-esi-template/cyclone-cases.json' help='Full path' width='50%_none'></param>
        <param name='_pw_load_openfoam' label='Command to Load OpenFOAM' type='text' value='spack load openfoam@2206%gcc@=10.4.0 arch=linux-rocky8-zen3' help='E.g.: module load, spack load, source, export PATH=, etc' width='50%_none'></param>
   </section>
    <section name='ofhost' type='section' title='Slurm configuration' expanded='true'>
        <param name='_pw_ofhost_jobschedulertype' label='Job Scheduler Type' type='hidden' value='SLURM' width='50%_none'>
        </param>
        <param name='_pw__ofhost__sch__dd_partition_e_' label='Slurm Partition' type='text' help='Name of the SLURM partition to run the OpenFOAM cases' value='normal' width='50%_none'>
        </param>
        <param name='_pw__ofhost__sch__dd_cpus_d_per_d_task_e_' label='CPUs per Task' type='integer' min="1" max="120" help='CPUs per Task to run each OpenFOAM case, --cpus-per-task=value slurm directive' value='60' width='50%_none'>
        </param>
        <param name='_pw__ofhost__sch__dd_time_e_' label='Walltime' type='text' help='Maximum walltime per OpenFOAM case' value='02:00:00' width='50%_none'>
        </param>
        <param name='_pw_ofhost_scheduler_directives' label='Scheduler Directives' type='text' help='e.g. --mem=1000;--gpus-per-node=1 - Use the semicolon character ; to separate parameters. Do not include the SBATCH keyword.' value='' width='100%_none'>
        </param>
    </section>
    <section name='advanced_options_other' type='section' title='Advanced Options' expanded='false'>
        <param name='_pw_controller' label='Controller host' type='text' value='pw.conf' help='Use hostname@ip or a proxy like PoolName.clusters.pw. Use pw.conf to get PoolName from pw.conf' width='50%_none'>
        </param>
        <param name='_pw_jobdir' label='Remote Job Directory' type='text' value='/scratch/__USER__/pw/jobs/__JOB_NUMBER__' help='Working directory of the batch script to directory before it is executed' width='50%_none'>
        </param>
    </section>
  </inputs>
  <outputs>
  </outputs>
</tool>