<tool id='User.Demo_say_hello' name='User.Demo_say_hello'>
  <command interpreter='bash'>main.sh</command>
  <inputs>
   <section name='openfoam' type='section' title='OpenFOAM' expanded='true'>
        <param name='cases_json_file' label='JSON cases definition in remote host' type='text' value='/home/__USER__/pworks/openfoam/cyclone-template/cyclone-cases.json' help='Full path' width='50%_none'></param>
        <param name='sif_file' label='Singularity container in remote host' type='text' value='/home/__USER__/pworks/openfoam/openfoam9-paraview56.sif' help='Full path' width='50%_none'></param>
        <param name='def_file' type='select' label='Singularity definition in bootstrap directory' help='If singularity container is missing it is built using this file' width='50%_none' multiple='false'>
            <option value="openfoam9-paraview56.def" selected="true">openfoam9-paraview56</option>
        </param>
   </section>
    <section name='host' type='section' title='Desktop Host' expanded='true'>
        <param name='partition' label='Slurm partition:' type='text' help='Partition to submit the interactive job to.' value='default' width='50%_none'>
        </param>
        <param name='numnodes' label='Number of nodes:' type='integer' min="1" max="10" help='Number of nodes to request for the interactive session.' value='1' width='50%_none'>
        </param>
        <param name="exclusive" type="boolean" truevalue="Yes" falsevalue="No" checked="True" label="Exclusive" help='The job allocation can not share nodes with other running jobs' width="25%_none" optional='true' float="right">
        </param>
        <param name='walltime' placeholder="test" type='text' help='e.g. 01:00:00 - Amount of time slurm will honor the interactive session.' value='01:00:00' width='50%_none'>
        </param>
    </section>
    <section name='advanced_options_other' type='section' title='Advanced Options' expanded='false'>
        <param name='controller' label='Controller host' type='text' value='pw.conf' help='Use hostname@ip or a proxy like PoolName.clusters.pw. Use pw.conf to get PoolName from pw.conf' width='50%_none'>
        </param>
        <param name='chdir' label='Working directory of the batch script' type='text' value='/home/\$(whoami)/pworks/__job_number__' help='Working directory of the batch script to directory before it is executed' width='50%_none'>
        </param>
    </section>
  </inputs>
  <outputs>
  </outputs>
</tool>